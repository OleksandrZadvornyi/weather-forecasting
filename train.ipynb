{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksandrZadvornyi/weather-forecasting/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdivx5UdGvLD",
        "outputId": "e0ea5a38-5ffa-4ea6-e27c-fd5d4b0df966"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q accelerate\n",
        "!pip install -q gluonts ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Eul86ODxGD5B"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from functools import lru_cache\n",
        "from functools import partial\n",
        "\n",
        "from gluonts.time_feature import (\n",
        "    get_lags_for_frequency,\n",
        "    time_features_from_frequency_str\n",
        ")\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "from gluonts.transform.sampler import InstanceSampler\n",
        "from typing import Optional, Iterable\n",
        "\n",
        "from gluonts.itertools import Cached, Cyclic\n",
        "from gluonts.dataset.loader import as_stacked_batches\n",
        "\n",
        "from gluonts.transform import (\n",
        "    AddAgeFeature,\n",
        "    AddObservedValuesIndicator,\n",
        "    AddTimeFeatures,\n",
        "    AsNumpyArray,\n",
        "    Chain,\n",
        "    ExpectedNumInstanceSampler,\n",
        "    InstanceSplitter,\n",
        "    RemoveFields,\n",
        "    TestSplitSampler,\n",
        "    Transformation,\n",
        "    ValidationSplitSampler,\n",
        "    VstackFeatures,\n",
        "    RenameFields,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    TimeSeriesTransformerConfig,\n",
        "    TimeSeriesTransformerForPrediction,\n",
        "    PretrainedConfig\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8_HZWvfPGHVH"
      },
      "outputs": [],
      "source": [
        "@lru_cache(10_000)\n",
        "def convert_to_pandas_period(date, freq):\n",
        "    return pd.Period(date, freq)\n",
        "\n",
        "def transform_start_field(batch, freq):\n",
        "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
        "    return batch\n",
        "\n",
        "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
        "    remove_field_names = []\n",
        "    if config.num_static_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
        "    if config.num_dynamic_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
        "    if config.num_static_categorical_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
        "\n",
        "    return Chain(\n",
        "        [RemoveFields(field_names=remove_field_names)]\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_CAT,\n",
        "                    expected_ndim=1,\n",
        "                    dtype=int,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_REAL,\n",
        "                    expected_ndim=1,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_real_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + [\n",
        "            AsNumpyArray(\n",
        "                field=FieldName.TARGET,\n",
        "                expected_ndim=1 if config.input_size == 1 else 2,\n",
        "            ),\n",
        "            AddObservedValuesIndicator(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.OBSERVED_VALUES,\n",
        "            ),\n",
        "            AddTimeFeatures(\n",
        "                start_field=FieldName.START,\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                time_features=time_features_from_frequency_str(freq),\n",
        "                pred_length=config.prediction_length,\n",
        "            ),\n",
        "            AddAgeFeature(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_AGE,\n",
        "                pred_length=config.prediction_length,\n",
        "                log_scale=True,\n",
        "            ),\n",
        "            VstackFeatures(\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
        "                + (\n",
        "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
        "                    if config.num_dynamic_real_features > 0\n",
        "                    else []\n",
        "                ),\n",
        "            ),\n",
        "            RenameFields(\n",
        "                mapping={\n",
        "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
        "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
        "                    FieldName.FEAT_TIME: \"time_features\",\n",
        "                    FieldName.TARGET: \"values\",\n",
        "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
        "                }\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def create_instance_splitter(\n",
        "    config: PretrainedConfig,\n",
        "    mode: str,\n",
        "    train_sampler: Optional[InstanceSampler] = None,\n",
        "    validation_sampler: Optional[InstanceSampler] = None,\n",
        ") -> Transformation:\n",
        "    assert mode in [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    instance_sampler = {\n",
        "        \"train\": train_sampler\n",
        "        or ExpectedNumInstanceSampler(\n",
        "            num_instances=1.0, min_future=config.prediction_length\n",
        "        ),\n",
        "        \"validation\": validation_sampler\n",
        "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
        "        \"test\": TestSplitSampler(),\n",
        "    }[mode]\n",
        "\n",
        "    return InstanceSplitter(\n",
        "        target_field=\"values\",\n",
        "        is_pad_field=FieldName.IS_PAD,\n",
        "        start_field=FieldName.START,\n",
        "        forecast_start_field=FieldName.FORECAST_START,\n",
        "        instance_sampler=instance_sampler,\n",
        "        past_length=config.context_length + max(config.lags_sequence),\n",
        "        future_length=config.prediction_length,\n",
        "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
        "    )\n",
        "\n",
        "def create_train_dataloader(\n",
        "    config: PretrainedConfig,\n",
        "    freq,\n",
        "    data,\n",
        "    batch_size: int,\n",
        "    num_batches_per_epoch: int,\n",
        "    shuffle_buffer_length: Optional[int] = None,\n",
        "    cache_data: bool = True,\n",
        "    **kwargs,\n",
        ") -> Iterable:\n",
        "    PREDICTION_INPUT_NAMES = [\n",
        "        \"past_time_features\",\n",
        "        \"past_values\",\n",
        "        \"past_observed_mask\",\n",
        "        \"future_time_features\",\n",
        "    ]\n",
        "    if config.num_static_categorical_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
        "\n",
        "    if config.num_static_real_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
        "\n",
        "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
        "        \"future_values\",\n",
        "        \"future_observed_mask\",\n",
        "    ]\n",
        "\n",
        "    transformation = create_transformation(freq, config)\n",
        "    transformed_data = transformation.apply(data, is_train=True)\n",
        "    if cache_data:\n",
        "        transformed_data = Cached(transformed_data)\n",
        "\n",
        "    instance_splitter = create_instance_splitter(config, \"train\")\n",
        "\n",
        "    stream = Cyclic(transformed_data).stream()\n",
        "    training_instances = instance_splitter.apply(stream)\n",
        "\n",
        "    return as_stacked_batches(\n",
        "        training_instances,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer_length=shuffle_buffer_length,\n",
        "        field_names=TRAINING_INPUT_NAMES,\n",
        "        output_type=torch.tensor,\n",
        "        num_batches_per_epoch=num_batches_per_epoch,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XzJ4hukGZlP",
        "outputId": "47535479-0997-4667-fe41-db7cdd78856b"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "data_dir = \"/content/drive/MyDrive/weather_forecasting_project/prepared_datasets\"\n",
        "\n",
        "dataset = load_from_disk(f\"{data_dir}/dataset\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "validation_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} time series\")\n",
        "print(f\"Validation dataset: {len(validation_dataset)} time series\")\n",
        "print(f\"Test dataset: {len(test_dataset)} time series\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QzBMKvMJF8uf"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "with open(f\"{data_dir}/metadata.txt\", \"r\") as f:\n",
        "    metadata = {}\n",
        "    for line in f:\n",
        "        key, value = line.strip().split(\"=\")\n",
        "        metadata[key] = value\n",
        "\n",
        "target_column = metadata.get(\"target_column\", \"TMAX\")\n",
        "freq = metadata.get(\"freq\", \"D\")  # Daily frequency\n",
        "\n",
        "# Example of a time series from training set\n",
        "train_example = train_dataset[0]\n",
        "validation_example = validation_dataset[0]\n",
        "test_example = test_dataset[0]\n",
        "\n",
        "# Define prediction parameters\n",
        "prediction_length = 180  # Predict 7 days ahead\n",
        "context_length = prediction_length * 2  # Use 14 days of context\n",
        "\n",
        "# assert len(train_example[\"target\"]) + prediction_length == len(\n",
        "#     validation_example[\"target\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XiQ_8_gdo2fL"
      },
      "outputs": [],
      "source": [
        "# Set transforms\n",
        "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "validation_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "test_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "\n",
        "# Configure model\n",
        "lags_sequence = get_lags_for_frequency(freq)\n",
        "time_features = time_features_from_frequency_str(freq)\n",
        "\n",
        "config = TimeSeriesTransformerConfig(\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    lags_sequence=lags_sequence,\n",
        "    num_time_features=len(time_features) + 1,  # Add 1 for age feature\n",
        "    num_static_categorical_features=1,\n",
        "    num_static_real_features=3,  # Latitude, longitude, elevation\n",
        "    cardinality=[len(train_dataset)],  # Number of unique time series\n",
        "    embedding_dimension=[2],  # Dimension of categorical embedding\n",
        "    encoder_layers=4,\n",
        "    decoder_layers=4,\n",
        "    d_model=64,\n",
        ")\n",
        "\n",
        "model = TimeSeriesTransformerForPrediction(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0wHTqUi4o0bl"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_dataloader = create_train_dataloader(\n",
        "    config=config,\n",
        "    freq=freq,\n",
        "    data=train_dataset,\n",
        "    batch_size=128,\n",
        "    num_batches_per_epoch=50,\n",
        ")\n",
        "\n",
        "# Set up training\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-2)\n",
        "\n",
        "model, optimizer, train_dataloader = accelerator.prepare(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L1BVdbLbowVx",
        "outputId": "0d6cecb9-22c2-4548-c86f-1199c44be061"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "num_epochs = 30\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else None,\n",
        "            static_real_features=batch[\"static_real_features\"].to(device)\n",
        "            if config.num_static_real_features > 0\n",
        "            else None,\n",
        "            past_time_features=batch[\"past_time_features\"].to(device),\n",
        "            past_values=batch[\"past_values\"].to(device),\n",
        "            future_time_features=batch[\"future_time_features\"].to(device),\n",
        "            future_values=batch[\"future_values\"].to(device),\n",
        "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
        "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch} completed. Average loss: {avg_epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh1H3glRVRKI",
        "outputId": "78188d51-2d40-477a-b4fa-90c05d2498c3"
      },
      "outputs": [],
      "source": [
        "# Save the model and configuration\n",
        "model_path = \"/content/drive/MyDrive/weather_forecasting_project/weather_model\"\n",
        "config_path = \"/content/drive/MyDrive/weather_forecasting_project/weather_model/config\"\n",
        "\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(config_path, exist_ok=True)\n",
        "\n",
        "# Get the unwrapped model if using accelerator\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "\n",
        "# Save model state dictionary\n",
        "torch.save(unwrapped_model.state_dict(), os.path.join(model_path, \"time_series_model.pth\"))\n",
        "\n",
        "# Save the configuration\n",
        "unwrapped_model.config.to_json_file(os.path.join(config_path, \"config.json\"))\n",
        "\n",
        "# Save frequency and prediction length for later use\n",
        "with open(os.path.join(config_path, \"metadata.txt\"), \"w\") as f:\n",
        "    f.write(f\"freq={freq}\\n\")\n",
        "    f.write(f\"prediction_length={prediction_length}\\n\")\n",
        "    f.write(f\"target_column={target_column}\\n\")\n",
        "    f.write(f\"lags_sequence={lags_sequence}\\n\")\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "print(f\"Configuration saved to {config_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
