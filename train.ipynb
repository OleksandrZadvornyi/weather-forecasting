{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksandrZadvornyi/weather-forecasting/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q accelerate\n",
        "!pip install -q gluonts ujson\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdivx5UdGvLD",
        "outputId": "e0ea5a38-5ffa-4ea6-e27c-fd5d4b0df966"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "from functools import lru_cache\n",
        "from functools import partial\n",
        "\n",
        "from gluonts.time_feature import (\n",
        "    get_lags_for_frequency,\n",
        "    time_features_from_frequency_str\n",
        ")\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "from gluonts.transform.sampler import InstanceSampler\n",
        "from typing import Optional, Iterable\n",
        "\n",
        "from gluonts.itertools import Cached, Cyclic\n",
        "from gluonts.dataset.loader import as_stacked_batches\n",
        "\n",
        "from gluonts.transform import (\n",
        "    AddAgeFeature,\n",
        "    AddObservedValuesIndicator,\n",
        "    AddTimeFeatures,\n",
        "    AsNumpyArray,\n",
        "    Chain,\n",
        "    ExpectedNumInstanceSampler,\n",
        "    InstanceSplitter,\n",
        "    RemoveFields,\n",
        "    TestSplitSampler,\n",
        "    Transformation,\n",
        "    ValidationSplitSampler,\n",
        "    VstackFeatures,\n",
        "    RenameFields,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    TimeSeriesTransformerConfig,\n",
        "    TimeSeriesTransformerForPrediction,\n",
        "    PretrainedConfig\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "Eul86ODxGD5B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(10_000)\n",
        "def convert_to_pandas_period(date, freq):\n",
        "    return pd.Period(date, freq)\n",
        "\n",
        "def transform_start_field(batch, freq):\n",
        "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
        "    return batch\n",
        "\n",
        "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
        "    remove_field_names = []\n",
        "    if config.num_static_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
        "    if config.num_dynamic_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
        "    if config.num_static_categorical_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
        "\n",
        "    return Chain(\n",
        "        [RemoveFields(field_names=remove_field_names)]\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_CAT,\n",
        "                    expected_ndim=1,\n",
        "                    dtype=int,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_REAL,\n",
        "                    expected_ndim=1,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_real_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + [\n",
        "            AsNumpyArray(\n",
        "                field=FieldName.TARGET,\n",
        "                expected_ndim=1 if config.input_size == 1 else 2,\n",
        "            ),\n",
        "            AddObservedValuesIndicator(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.OBSERVED_VALUES,\n",
        "            ),\n",
        "            AddTimeFeatures(\n",
        "                start_field=FieldName.START,\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                time_features=time_features_from_frequency_str(freq),\n",
        "                pred_length=config.prediction_length,\n",
        "            ),\n",
        "            AddAgeFeature(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_AGE,\n",
        "                pred_length=config.prediction_length,\n",
        "                log_scale=True,\n",
        "            ),\n",
        "            VstackFeatures(\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
        "                + (\n",
        "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
        "                    if config.num_dynamic_real_features > 0\n",
        "                    else []\n",
        "                ),\n",
        "            ),\n",
        "            RenameFields(\n",
        "                mapping={\n",
        "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
        "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
        "                    FieldName.FEAT_TIME: \"time_features\",\n",
        "                    FieldName.TARGET: \"values\",\n",
        "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
        "                }\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def create_instance_splitter(\n",
        "    config: PretrainedConfig,\n",
        "    mode: str,\n",
        "    train_sampler: Optional[InstanceSampler] = None,\n",
        "    validation_sampler: Optional[InstanceSampler] = None,\n",
        ") -> Transformation:\n",
        "    assert mode in [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    instance_sampler = {\n",
        "        \"train\": train_sampler\n",
        "        or ExpectedNumInstanceSampler(\n",
        "            num_instances=1.0, min_future=config.prediction_length\n",
        "        ),\n",
        "        \"validation\": validation_sampler\n",
        "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
        "        \"test\": TestSplitSampler(),\n",
        "    }[mode]\n",
        "\n",
        "    return InstanceSplitter(\n",
        "        target_field=\"values\",\n",
        "        is_pad_field=FieldName.IS_PAD,\n",
        "        start_field=FieldName.START,\n",
        "        forecast_start_field=FieldName.FORECAST_START,\n",
        "        instance_sampler=instance_sampler,\n",
        "        past_length=config.context_length + max(config.lags_sequence),\n",
        "        future_length=config.prediction_length,\n",
        "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
        "    )\n",
        "\n",
        "def create_train_dataloader(\n",
        "    config: PretrainedConfig,\n",
        "    freq,\n",
        "    data,\n",
        "    batch_size: int,\n",
        "    num_batches_per_epoch: int,\n",
        "    shuffle_buffer_length: Optional[int] = None,\n",
        "    cache_data: bool = True,\n",
        "    **kwargs,\n",
        ") -> Iterable:\n",
        "    PREDICTION_INPUT_NAMES = [\n",
        "        \"past_time_features\",\n",
        "        \"past_values\",\n",
        "        \"past_observed_mask\",\n",
        "        \"future_time_features\",\n",
        "    ]\n",
        "    if config.num_static_categorical_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
        "\n",
        "    if config.num_static_real_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
        "\n",
        "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
        "        \"future_values\",\n",
        "        \"future_observed_mask\",\n",
        "    ]\n",
        "\n",
        "    transformation = create_transformation(freq, config)\n",
        "    transformed_data = transformation.apply(data, is_train=True)\n",
        "    if cache_data:\n",
        "        transformed_data = Cached(transformed_data)\n",
        "\n",
        "    instance_splitter = create_instance_splitter(config, \"train\")\n",
        "\n",
        "    stream = Cyclic(transformed_data).stream()\n",
        "    training_instances = instance_splitter.apply(stream)\n",
        "\n",
        "    return as_stacked_batches(\n",
        "        training_instances,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer_length=shuffle_buffer_length,\n",
        "        field_names=TRAINING_INPUT_NAMES,\n",
        "        output_type=torch.tensor,\n",
        "        num_batches_per_epoch=num_batches_per_epoch,\n",
        "    )"
      ],
      "metadata": {
        "id": "8_HZWvfPGHVH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "data_dir = \"/content/drive/MyDrive/weather_forecasting_project/prepared_datasets\"\n",
        "\n",
        "dataset = load_from_disk(f\"{data_dir}/dataset\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "validation_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} time series\")\n",
        "print(f\"Validation dataset: {len(validation_dataset)} time series\")\n",
        "print(f\"Test dataset: {len(test_dataset)} time series\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XzJ4hukGZlP",
        "outputId": "47535479-0997-4667-fe41-db7cdd78856b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 19 time series\n",
            "Validation dataset: 19 time series\n",
            "Test dataset: 19 time series\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QzBMKvMJF8uf"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "with open(f\"{data_dir}/metadata.txt\", \"r\") as f:\n",
        "    metadata = {}\n",
        "    for line in f:\n",
        "        key, value = line.strip().split(\"=\")\n",
        "        metadata[key] = value\n",
        "\n",
        "target_column = metadata.get(\"target_column\", \"TMAX\")\n",
        "freq = metadata.get(\"freq\", \"D\")  # Daily frequency\n",
        "\n",
        "# Example of a time series from training set\n",
        "train_example = train_dataset[0]\n",
        "validation_example = validation_dataset[0]\n",
        "test_example = test_dataset[0]\n",
        "\n",
        "# Define prediction parameters\n",
        "prediction_length = 180  # Predict 7 days ahead\n",
        "context_length = prediction_length * 2  # Use 14 days of context\n",
        "\n",
        "# assert len(train_example[\"target\"]) + prediction_length == len(\n",
        "#     validation_example[\"target\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transforms\n",
        "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "validation_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "test_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "\n",
        "# Configure model\n",
        "lags_sequence = get_lags_for_frequency(freq)\n",
        "time_features = time_features_from_frequency_str(freq)\n",
        "\n",
        "config = TimeSeriesTransformerConfig(\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    lags_sequence=lags_sequence,\n",
        "    num_time_features=len(time_features) + 1,  # Add 1 for age feature\n",
        "    num_static_categorical_features=1,\n",
        "    num_static_real_features=3,  # Latitude, longitude, elevation\n",
        "    cardinality=[len(train_dataset)],  # Number of unique time series\n",
        "    embedding_dimension=[2],  # Dimension of categorical embedding\n",
        "    encoder_layers=4,\n",
        "    decoder_layers=4,\n",
        "    d_model=64,\n",
        ")\n",
        "\n",
        "model = TimeSeriesTransformerForPrediction(config)"
      ],
      "metadata": {
        "id": "XiQ_8_gdo2fL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_dataloader = create_train_dataloader(\n",
        "    config=config,\n",
        "    freq=freq,\n",
        "    data=train_dataset,\n",
        "    batch_size=128,\n",
        "    num_batches_per_epoch=50,\n",
        ")\n",
        "\n",
        "# Set up training\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-2)\n",
        "\n",
        "model, optimizer, train_dataloader = accelerator.prepare(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        ")"
      ],
      "metadata": {
        "id": "0wHTqUi4o0bl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "num_epochs = 30\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else None,\n",
        "            static_real_features=batch[\"static_real_features\"].to(device)\n",
        "            if config.num_static_real_features > 0\n",
        "            else None,\n",
        "            past_time_features=batch[\"past_time_features\"].to(device),\n",
        "            past_values=batch[\"past_values\"].to(device),\n",
        "            future_time_features=batch[\"future_time_features\"].to(device),\n",
        "            future_values=batch[\"future_values\"].to(device),\n",
        "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
        "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch} completed. Average loss: {avg_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L1BVdbLbowVx",
        "outputId": "0d6cecb9-22c2-4548-c86f-1199c44be061"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 30 epochs...\n",
            "Epoch 0, Batch 0, Loss: 6.5456\n",
            "Epoch 0, Batch 10, Loss: 6.4243\n",
            "Epoch 0, Batch 20, Loss: 6.3129\n",
            "Epoch 0, Batch 30, Loss: 6.3074\n",
            "Epoch 0, Batch 40, Loss: 6.2391\n",
            "Epoch 0 completed. Average loss: 6.3413\n",
            "Epoch 1, Batch 0, Loss: 6.2461\n",
            "Epoch 1, Batch 10, Loss: 6.2041\n",
            "Epoch 1, Batch 20, Loss: 6.1512\n",
            "Epoch 1, Batch 30, Loss: 6.1204\n",
            "Epoch 1, Batch 40, Loss: 6.1497\n",
            "Epoch 1 completed. Average loss: 6.1589\n",
            "Epoch 2, Batch 0, Loss: 6.1025\n",
            "Epoch 2, Batch 10, Loss: 6.0422\n",
            "Epoch 2, Batch 20, Loss: 6.0220\n",
            "Epoch 2, Batch 30, Loss: 5.9534\n",
            "Epoch 2, Batch 40, Loss: 5.8874\n",
            "Epoch 2 completed. Average loss: 5.9625\n",
            "Epoch 3, Batch 0, Loss: 5.7967\n",
            "Epoch 3, Batch 10, Loss: 5.7587\n",
            "Epoch 3, Batch 20, Loss: 5.7652\n",
            "Epoch 3, Batch 30, Loss: 5.6834\n",
            "Epoch 3, Batch 40, Loss: 5.5966\n",
            "Epoch 3 completed. Average loss: 5.6949\n",
            "Epoch 4, Batch 0, Loss: 5.5604\n",
            "Epoch 4, Batch 10, Loss: 5.5584\n",
            "Epoch 4, Batch 20, Loss: 5.5031\n",
            "Epoch 4, Batch 30, Loss: 5.4757\n",
            "Epoch 4, Batch 40, Loss: 5.5303\n",
            "Epoch 4 completed. Average loss: 5.4892\n",
            "Epoch 5, Batch 0, Loss: 5.4144\n",
            "Epoch 5, Batch 10, Loss: 5.3657\n",
            "Epoch 5, Batch 20, Loss: 5.3943\n",
            "Epoch 5, Batch 30, Loss: 5.3318\n",
            "Epoch 5, Batch 40, Loss: 5.2713\n",
            "Epoch 5 completed. Average loss: 5.3440\n",
            "Epoch 6, Batch 0, Loss: 5.2588\n",
            "Epoch 6, Batch 10, Loss: 5.2300\n",
            "Epoch 6, Batch 20, Loss: 5.2151\n",
            "Epoch 6, Batch 30, Loss: 5.1982\n",
            "Epoch 6, Batch 40, Loss: 5.2022\n",
            "Epoch 6 completed. Average loss: 5.2432\n",
            "Epoch 7, Batch 0, Loss: 5.2968\n",
            "Epoch 7, Batch 10, Loss: 5.1497\n",
            "Epoch 7, Batch 20, Loss: 5.1702\n",
            "Epoch 7, Batch 30, Loss: 5.1632\n",
            "Epoch 7, Batch 40, Loss: 5.1010\n",
            "Epoch 7 completed. Average loss: 5.1716\n",
            "Epoch 8, Batch 0, Loss: 5.1106\n",
            "Epoch 8, Batch 10, Loss: 5.1501\n",
            "Epoch 8, Batch 20, Loss: 5.0951\n",
            "Epoch 8, Batch 30, Loss: 5.1160\n",
            "Epoch 8, Batch 40, Loss: 5.1892\n",
            "Epoch 8 completed. Average loss: 5.1298\n",
            "Epoch 9, Batch 0, Loss: 5.0839\n",
            "Epoch 9, Batch 10, Loss: 5.0695\n",
            "Epoch 9, Batch 20, Loss: 5.0682\n",
            "Epoch 9, Batch 30, Loss: 5.0349\n",
            "Epoch 9, Batch 40, Loss: 5.0531\n",
            "Epoch 9 completed. Average loss: 5.0715\n",
            "Epoch 10, Batch 0, Loss: 5.0225\n",
            "Epoch 10, Batch 10, Loss: 5.0379\n",
            "Epoch 10, Batch 20, Loss: 5.0315\n",
            "Epoch 10, Batch 30, Loss: 4.9962\n",
            "Epoch 10, Batch 40, Loss: 5.0066\n",
            "Epoch 10 completed. Average loss: 5.0248\n",
            "Epoch 11, Batch 0, Loss: 4.9599\n",
            "Epoch 11, Batch 10, Loss: 5.0244\n",
            "Epoch 11, Batch 20, Loss: 4.9996\n",
            "Epoch 11, Batch 30, Loss: 5.0219\n",
            "Epoch 11, Batch 40, Loss: 4.9349\n",
            "Epoch 11 completed. Average loss: 4.9986\n",
            "Epoch 12, Batch 0, Loss: 5.0287\n",
            "Epoch 12, Batch 10, Loss: 5.0495\n",
            "Epoch 12, Batch 20, Loss: 5.0391\n",
            "Epoch 12, Batch 30, Loss: 4.9426\n",
            "Epoch 12, Batch 40, Loss: 4.9347\n",
            "Epoch 12 completed. Average loss: 4.9806\n",
            "Epoch 13, Batch 0, Loss: 4.8979\n",
            "Epoch 13, Batch 10, Loss: 4.9970\n",
            "Epoch 13, Batch 20, Loss: 4.9770\n",
            "Epoch 13, Batch 30, Loss: 4.9650\n",
            "Epoch 13, Batch 40, Loss: 4.9273\n",
            "Epoch 13 completed. Average loss: 4.9430\n",
            "Epoch 14, Batch 0, Loss: 4.9932\n",
            "Epoch 14, Batch 10, Loss: 4.8965\n",
            "Epoch 14, Batch 20, Loss: 4.9561\n",
            "Epoch 14, Batch 30, Loss: 4.9142\n",
            "Epoch 14, Batch 40, Loss: 4.9033\n",
            "Epoch 14 completed. Average loss: 4.9335\n",
            "Epoch 15, Batch 0, Loss: 4.9244\n",
            "Epoch 15, Batch 10, Loss: 4.8762\n",
            "Epoch 15, Batch 20, Loss: 4.9138\n",
            "Epoch 15, Batch 30, Loss: 4.8825\n",
            "Epoch 15, Batch 40, Loss: 4.9064\n",
            "Epoch 15 completed. Average loss: 4.9050\n",
            "Epoch 16, Batch 0, Loss: 4.8877\n",
            "Epoch 16, Batch 10, Loss: 4.8978\n",
            "Epoch 16, Batch 20, Loss: 4.8718\n",
            "Epoch 16, Batch 30, Loss: 4.8393\n",
            "Epoch 16, Batch 40, Loss: 4.8372\n",
            "Epoch 16 completed. Average loss: 4.8929\n",
            "Epoch 17, Batch 0, Loss: 4.9264\n",
            "Epoch 17, Batch 10, Loss: 4.9316\n",
            "Epoch 17, Batch 20, Loss: 4.9153\n",
            "Epoch 17, Batch 30, Loss: 4.8521\n",
            "Epoch 17, Batch 40, Loss: 4.8301\n",
            "Epoch 17 completed. Average loss: 4.8828\n",
            "Epoch 18, Batch 0, Loss: 4.8958\n",
            "Epoch 18, Batch 10, Loss: 4.8358\n",
            "Epoch 18, Batch 20, Loss: 4.8664\n",
            "Epoch 18, Batch 30, Loss: 4.8638\n",
            "Epoch 18, Batch 40, Loss: 4.8687\n",
            "Epoch 18 completed. Average loss: 4.8567\n",
            "Epoch 19, Batch 0, Loss: 4.9091\n",
            "Epoch 19, Batch 10, Loss: 4.8535\n",
            "Epoch 19, Batch 20, Loss: 4.8178\n",
            "Epoch 19, Batch 30, Loss: 4.8315\n",
            "Epoch 19, Batch 40, Loss: 4.8220\n",
            "Epoch 19 completed. Average loss: 4.8494\n",
            "Epoch 20, Batch 0, Loss: 4.8090\n",
            "Epoch 20, Batch 10, Loss: 4.8476\n",
            "Epoch 20, Batch 20, Loss: 4.8258\n",
            "Epoch 20, Batch 30, Loss: 4.8410\n",
            "Epoch 20, Batch 40, Loss: 4.7827\n",
            "Epoch 20 completed. Average loss: 4.8397\n",
            "Epoch 21, Batch 0, Loss: 4.8740\n",
            "Epoch 21, Batch 10, Loss: 4.8194\n",
            "Epoch 21, Batch 20, Loss: 4.8898\n",
            "Epoch 21, Batch 30, Loss: 4.8136\n",
            "Epoch 21, Batch 40, Loss: 4.7918\n",
            "Epoch 21 completed. Average loss: 4.8335\n",
            "Epoch 22, Batch 0, Loss: 4.8420\n",
            "Epoch 22, Batch 10, Loss: 4.8450\n",
            "Epoch 22, Batch 20, Loss: 4.7932\n",
            "Epoch 22, Batch 30, Loss: 4.7911\n",
            "Epoch 22, Batch 40, Loss: 4.8328\n",
            "Epoch 22 completed. Average loss: 4.8140\n",
            "Epoch 23, Batch 0, Loss: 4.8556\n",
            "Epoch 23, Batch 10, Loss: 4.7777\n",
            "Epoch 23, Batch 20, Loss: 4.7696\n",
            "Epoch 23, Batch 30, Loss: 4.7698\n",
            "Epoch 23, Batch 40, Loss: 4.7863\n",
            "Epoch 23 completed. Average loss: 4.7986\n",
            "Epoch 24, Batch 0, Loss: 4.8275\n",
            "Epoch 24, Batch 10, Loss: 4.7489\n",
            "Epoch 24, Batch 20, Loss: 4.7839\n",
            "Epoch 24, Batch 30, Loss: 4.8090\n",
            "Epoch 24, Batch 40, Loss: 4.8051\n",
            "Epoch 24 completed. Average loss: 4.7926\n",
            "Epoch 25, Batch 0, Loss: 4.7514\n",
            "Epoch 25, Batch 10, Loss: 4.7781\n",
            "Epoch 25, Batch 20, Loss: 4.7466\n",
            "Epoch 25, Batch 30, Loss: 4.8125\n",
            "Epoch 25, Batch 40, Loss: 4.8387\n",
            "Epoch 25 completed. Average loss: 4.7775\n",
            "Epoch 26, Batch 0, Loss: 4.7660\n",
            "Epoch 26, Batch 10, Loss: 4.8077\n",
            "Epoch 26, Batch 20, Loss: 4.7713\n",
            "Epoch 26, Batch 30, Loss: 4.7846\n",
            "Epoch 26, Batch 40, Loss: 4.7541\n",
            "Epoch 26 completed. Average loss: 4.7655\n",
            "Epoch 27, Batch 0, Loss: 4.8024\n",
            "Epoch 27, Batch 10, Loss: 4.7489\n",
            "Epoch 27, Batch 20, Loss: 4.7474\n",
            "Epoch 27, Batch 30, Loss: 4.7089\n",
            "Epoch 27, Batch 40, Loss: 4.7483\n",
            "Epoch 27 completed. Average loss: 4.7578\n",
            "Epoch 28, Batch 0, Loss: 4.7260\n",
            "Epoch 28, Batch 10, Loss: 4.7756\n",
            "Epoch 28, Batch 20, Loss: 4.8131\n",
            "Epoch 28, Batch 30, Loss: 4.6877\n",
            "Epoch 28, Batch 40, Loss: 4.7331\n",
            "Epoch 28 completed. Average loss: 4.7429\n",
            "Epoch 29, Batch 0, Loss: 4.7272\n",
            "Epoch 29, Batch 10, Loss: 4.7410\n",
            "Epoch 29, Batch 20, Loss: 4.7833\n",
            "Epoch 29, Batch 30, Loss: 4.8063\n",
            "Epoch 29, Batch 40, Loss: 4.7540\n",
            "Epoch 29 completed. Average loss: 4.7567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and configuration\n",
        "model_path = \"/content/drive/MyDrive/weather_forecasting_project/weather_model\"\n",
        "config_path = \"/content/drive/MyDrive/weather_forecasting_project/weather_model/config\"\n",
        "\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(config_path, exist_ok=True)\n",
        "\n",
        "# Get the unwrapped model if using accelerator\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "\n",
        "# Save model state dictionary\n",
        "torch.save(unwrapped_model.state_dict(), os.path.join(model_path, \"time_series_model.pth\"))\n",
        "\n",
        "# Save the configuration\n",
        "unwrapped_model.config.to_json_file(os.path.join(config_path, \"config.json\"))\n",
        "\n",
        "# Save frequency and prediction length for later use\n",
        "with open(os.path.join(config_path, \"metadata.txt\"), \"w\") as f:\n",
        "    f.write(f\"freq={freq}\\n\")\n",
        "    f.write(f\"prediction_length={prediction_length}\\n\")\n",
        "    f.write(f\"target_column={target_column}\\n\")\n",
        "    f.write(f\"lags_sequence={lags_sequence}\\n\")\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "print(f\"Configuration saved to {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh1H3glRVRKI",
        "outputId": "78188d51-2d40-477a-b4fa-90c05d2498c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/weather_forecasting_project/weather_model\n",
            "Configuration saved to /content/drive/MyDrive/weather_forecasting_project/weather_model/config\n"
          ]
        }
      ]
    }
  ]
}