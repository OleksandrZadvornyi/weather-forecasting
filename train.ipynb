{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q accelerate\n",
        "!pip install -q gluonts ujson\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdivx5UdGvLD",
        "outputId": "254b29fc-2ce4-42dc-8aba-de3e21085d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "from functools import lru_cache\n",
        "from functools import partial\n",
        "\n",
        "from gluonts.time_feature import (\n",
        "    get_lags_for_frequency,\n",
        "    time_features_from_frequency_str\n",
        ")\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "from gluonts.transform.sampler import InstanceSampler\n",
        "from typing import Optional, Iterable\n",
        "\n",
        "from gluonts.itertools import Cached, Cyclic\n",
        "from gluonts.dataset.loader import as_stacked_batches\n",
        "\n",
        "from gluonts.transform import (\n",
        "    AddAgeFeature,\n",
        "    AddObservedValuesIndicator,\n",
        "    AddTimeFeatures,\n",
        "    AsNumpyArray,\n",
        "    Chain,\n",
        "    ExpectedNumInstanceSampler,\n",
        "    InstanceSplitter,\n",
        "    RemoveFields,\n",
        "    TestSplitSampler,\n",
        "    Transformation,\n",
        "    ValidationSplitSampler,\n",
        "    VstackFeatures,\n",
        "    RenameFields,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    TimeSeriesTransformerConfig,\n",
        "    TimeSeriesTransformerForPrediction,\n",
        "    PretrainedConfig\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "Eul86ODxGD5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache(10_000)\n",
        "def convert_to_pandas_period(date, freq):\n",
        "    return pd.Period(date, freq)\n",
        "\n",
        "def transform_start_field(batch, freq):\n",
        "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
        "    return batch\n",
        "\n",
        "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
        "    remove_field_names = []\n",
        "    if config.num_static_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
        "    if config.num_dynamic_real_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
        "    if config.num_static_categorical_features == 0:\n",
        "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
        "\n",
        "    return Chain(\n",
        "        [RemoveFields(field_names=remove_field_names)]\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_CAT,\n",
        "                    expected_ndim=1,\n",
        "                    dtype=int,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + (\n",
        "            [\n",
        "                AsNumpyArray(\n",
        "                    field=FieldName.FEAT_STATIC_REAL,\n",
        "                    expected_ndim=1,\n",
        "                )\n",
        "            ]\n",
        "            if config.num_static_real_features > 0\n",
        "            else []\n",
        "        )\n",
        "        + [\n",
        "            AsNumpyArray(\n",
        "                field=FieldName.TARGET,\n",
        "                expected_ndim=1 if config.input_size == 1 else 2,\n",
        "            ),\n",
        "            AddObservedValuesIndicator(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.OBSERVED_VALUES,\n",
        "            ),\n",
        "            AddTimeFeatures(\n",
        "                start_field=FieldName.START,\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                time_features=time_features_from_frequency_str(freq),\n",
        "                pred_length=config.prediction_length,\n",
        "            ),\n",
        "            AddAgeFeature(\n",
        "                target_field=FieldName.TARGET,\n",
        "                output_field=FieldName.FEAT_AGE,\n",
        "                pred_length=config.prediction_length,\n",
        "                log_scale=True,\n",
        "            ),\n",
        "            VstackFeatures(\n",
        "                output_field=FieldName.FEAT_TIME,\n",
        "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
        "                + (\n",
        "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
        "                    if config.num_dynamic_real_features > 0\n",
        "                    else []\n",
        "                ),\n",
        "            ),\n",
        "            RenameFields(\n",
        "                mapping={\n",
        "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
        "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
        "                    FieldName.FEAT_TIME: \"time_features\",\n",
        "                    FieldName.TARGET: \"values\",\n",
        "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
        "                }\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def create_instance_splitter(\n",
        "    config: PretrainedConfig,\n",
        "    mode: str,\n",
        "    train_sampler: Optional[InstanceSampler] = None,\n",
        "    validation_sampler: Optional[InstanceSampler] = None,\n",
        ") -> Transformation:\n",
        "    assert mode in [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    instance_sampler = {\n",
        "        \"train\": train_sampler\n",
        "        or ExpectedNumInstanceSampler(\n",
        "            num_instances=1.0, min_future=config.prediction_length\n",
        "        ),\n",
        "        \"validation\": validation_sampler\n",
        "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
        "        \"test\": TestSplitSampler(),\n",
        "    }[mode]\n",
        "\n",
        "    return InstanceSplitter(\n",
        "        target_field=\"values\",\n",
        "        is_pad_field=FieldName.IS_PAD,\n",
        "        start_field=FieldName.START,\n",
        "        forecast_start_field=FieldName.FORECAST_START,\n",
        "        instance_sampler=instance_sampler,\n",
        "        past_length=config.context_length + max(config.lags_sequence),\n",
        "        future_length=config.prediction_length,\n",
        "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
        "    )\n",
        "\n",
        "def create_train_dataloader(\n",
        "    config: PretrainedConfig,\n",
        "    freq,\n",
        "    data,\n",
        "    batch_size: int,\n",
        "    num_batches_per_epoch: int,\n",
        "    shuffle_buffer_length: Optional[int] = None,\n",
        "    cache_data: bool = True,\n",
        "    **kwargs,\n",
        ") -> Iterable:\n",
        "    PREDICTION_INPUT_NAMES = [\n",
        "        \"past_time_features\",\n",
        "        \"past_values\",\n",
        "        \"past_observed_mask\",\n",
        "        \"future_time_features\",\n",
        "    ]\n",
        "    if config.num_static_categorical_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
        "\n",
        "    if config.num_static_real_features > 0:\n",
        "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
        "\n",
        "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
        "        \"future_values\",\n",
        "        \"future_observed_mask\",\n",
        "    ]\n",
        "\n",
        "    transformation = create_transformation(freq, config)\n",
        "    transformed_data = transformation.apply(data, is_train=True)\n",
        "    if cache_data:\n",
        "        transformed_data = Cached(transformed_data)\n",
        "\n",
        "    instance_splitter = create_instance_splitter(config, \"train\")\n",
        "\n",
        "    stream = Cyclic(transformed_data).stream()\n",
        "    training_instances = instance_splitter.apply(stream)\n",
        "\n",
        "    return as_stacked_batches(\n",
        "        training_instances,\n",
        "        batch_size=batch_size,\n",
        "        shuffle_buffer_length=shuffle_buffer_length,\n",
        "        field_names=TRAINING_INPUT_NAMES,\n",
        "        output_type=torch.tensor,\n",
        "        num_batches_per_epoch=num_batches_per_epoch,\n",
        "    )"
      ],
      "metadata": {
        "id": "8_HZWvfPGHVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/prepared_datasets\"\n",
        "\n",
        "dataset = load_from_disk(f\"{data_dir}/dataset\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "validation_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} time series\")\n",
        "print(f\"Validation dataset: {len(validation_dataset)} time series\")\n",
        "print(f\"Test dataset: {len(test_dataset)} time series\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XzJ4hukGZlP",
        "outputId": "12cdd5c4-d9ff-445f-a3fb-1a3322c6d5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 19 time series\n",
            "Validation dataset: 19 time series\n",
            "Test dataset: 19 time series\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzBMKvMJF8uf"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "with open(f\"{data_dir}/metadata.txt\", \"r\") as f:\n",
        "    metadata = {}\n",
        "    for line in f:\n",
        "        key, value = line.strip().split(\"=\")\n",
        "        metadata[key] = value\n",
        "\n",
        "target_column = metadata.get(\"target_column\", \"TMAX\")\n",
        "freq = metadata.get(\"freq\", \"D\")  # Daily frequency\n",
        "\n",
        "# Example of a time series from training set\n",
        "train_example = train_dataset[0]\n",
        "validation_example = validation_dataset[0]\n",
        "test_example = test_dataset[0]\n",
        "\n",
        "# Define prediction parameters\n",
        "prediction_length = 180  # Predict 7 days ahead\n",
        "context_length = prediction_length * 2  # Use 14 days of context\n",
        "\n",
        "# assert len(train_example[\"target\"]) + prediction_length == len(\n",
        "#     validation_example[\"target\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transforms\n",
        "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "validation_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "test_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "\n",
        "# Configure model\n",
        "lags_sequence = get_lags_for_frequency(freq)\n",
        "time_features = time_features_from_frequency_str(freq)\n",
        "\n",
        "config = TimeSeriesTransformerConfig(\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    lags_sequence=lags_sequence,\n",
        "    num_time_features=len(time_features) + 1,  # Add 1 for age feature\n",
        "    num_static_categorical_features=1,\n",
        "    num_static_real_features=3,  # Latitude, longitude, elevation\n",
        "    cardinality=[len(train_dataset)],  # Number of unique time series\n",
        "    embedding_dimension=[2],  # Dimension of categorical embedding\n",
        "    encoder_layers=4,\n",
        "    decoder_layers=4,\n",
        "    d_model=64,\n",
        ")\n",
        "\n",
        "model = TimeSeriesTransformerForPrediction(config)"
      ],
      "metadata": {
        "id": "XiQ_8_gdo2fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_dataloader = create_train_dataloader(\n",
        "    config=config,\n",
        "    freq=freq,\n",
        "    data=train_dataset,\n",
        "    batch_size=128,\n",
        "    num_batches_per_epoch=50,\n",
        ")\n",
        "\n",
        "# Set up training\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-2)\n",
        "\n",
        "model, optimizer, train_dataloader = accelerator.prepare(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        ")"
      ],
      "metadata": {
        "id": "0wHTqUi4o0bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "num_epochs = 30\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
        "            if config.num_static_categorical_features > 0\n",
        "            else None,\n",
        "            static_real_features=batch[\"static_real_features\"].to(device)\n",
        "            if config.num_static_real_features > 0\n",
        "            else None,\n",
        "            past_time_features=batch[\"past_time_features\"].to(device),\n",
        "            past_values=batch[\"past_values\"].to(device),\n",
        "            future_time_features=batch[\"future_time_features\"].to(device),\n",
        "            future_values=batch[\"future_values\"].to(device),\n",
        "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
        "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch} completed. Average loss: {avg_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L1BVdbLbowVx",
        "outputId": "c4dadbc7-4180-4895-c84d-86422d0e5728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 30 epochs...\n",
            "Epoch 0, Batch 0, Loss: 6.8041\n",
            "Epoch 0, Batch 10, Loss: 6.6772\n",
            "Epoch 0, Batch 20, Loss: 6.5161\n",
            "Epoch 0, Batch 30, Loss: 6.3431\n",
            "Epoch 0, Batch 40, Loss: 6.3113\n",
            "Epoch 0 completed. Average loss: 6.4727\n",
            "Epoch 1, Batch 0, Loss: 6.2526\n",
            "Epoch 1, Batch 10, Loss: 6.2321\n",
            "Epoch 1, Batch 20, Loss: 6.1657\n",
            "Epoch 1, Batch 30, Loss: 6.1859\n",
            "Epoch 1, Batch 40, Loss: 6.2122\n",
            "Epoch 1 completed. Average loss: 6.1879\n",
            "Epoch 2, Batch 0, Loss: 6.1556\n",
            "Epoch 2, Batch 10, Loss: 6.0911\n",
            "Epoch 2, Batch 20, Loss: 6.0380\n",
            "Epoch 2, Batch 30, Loss: 5.9914\n",
            "Epoch 2, Batch 40, Loss: 6.0246\n",
            "Epoch 2 completed. Average loss: 6.0397\n",
            "Epoch 3, Batch 0, Loss: 5.8802\n",
            "Epoch 3, Batch 10, Loss: 5.8161\n",
            "Epoch 3, Batch 20, Loss: 5.7997\n",
            "Epoch 3, Batch 30, Loss: 5.7521\n",
            "Epoch 3, Batch 40, Loss: 5.6588\n",
            "Epoch 3 completed. Average loss: 5.7738\n",
            "Epoch 4, Batch 0, Loss: 5.6602\n",
            "Epoch 4, Batch 10, Loss: 5.6620\n",
            "Epoch 4, Batch 20, Loss: 5.5453\n",
            "Epoch 4, Batch 30, Loss: 5.5539\n",
            "Epoch 4, Batch 40, Loss: 5.5447\n",
            "Epoch 4 completed. Average loss: 5.5632\n",
            "Epoch 5, Batch 0, Loss: 5.4251\n",
            "Epoch 5, Batch 10, Loss: 5.4729\n",
            "Epoch 5, Batch 20, Loss: 5.3827\n",
            "Epoch 5, Batch 30, Loss: 5.3563\n",
            "Epoch 5, Batch 40, Loss: 5.3431\n",
            "Epoch 5 completed. Average loss: 5.4042\n",
            "Epoch 6, Batch 0, Loss: 5.3211\n",
            "Epoch 6, Batch 10, Loss: 5.2716\n",
            "Epoch 6, Batch 20, Loss: 5.2643\n",
            "Epoch 6, Batch 30, Loss: 5.2905\n",
            "Epoch 6, Batch 40, Loss: 5.3371\n",
            "Epoch 6 completed. Average loss: 5.2946\n",
            "Epoch 7, Batch 0, Loss: 5.2046\n",
            "Epoch 7, Batch 10, Loss: 5.2557\n",
            "Epoch 7, Batch 20, Loss: 5.2379\n",
            "Epoch 7, Batch 30, Loss: 5.1915\n",
            "Epoch 7, Batch 40, Loss: 5.2082\n",
            "Epoch 7 completed. Average loss: 5.2238\n",
            "Epoch 8, Batch 0, Loss: 5.1547\n",
            "Epoch 8, Batch 10, Loss: 5.2107\n",
            "Epoch 8, Batch 20, Loss: 5.1648\n",
            "Epoch 8, Batch 30, Loss: 5.1333\n",
            "Epoch 8, Batch 40, Loss: 5.1038\n",
            "Epoch 8 completed. Average loss: 5.1526\n",
            "Epoch 9, Batch 0, Loss: 5.0875\n",
            "Epoch 9, Batch 10, Loss: 5.1166\n",
            "Epoch 9, Batch 20, Loss: 5.1462\n",
            "Epoch 9, Batch 30, Loss: 5.0503\n",
            "Epoch 9, Batch 40, Loss: 5.0163\n",
            "Epoch 9 completed. Average loss: 5.0962\n",
            "Epoch 10, Batch 0, Loss: 5.0654\n",
            "Epoch 10, Batch 10, Loss: 5.1240\n",
            "Epoch 10, Batch 20, Loss: 5.0123\n",
            "Epoch 10, Batch 30, Loss: 5.0870\n",
            "Epoch 10, Batch 40, Loss: 5.0008\n",
            "Epoch 10 completed. Average loss: 5.0550\n",
            "Epoch 11, Batch 0, Loss: 5.0393\n",
            "Epoch 11, Batch 10, Loss: 4.9915\n",
            "Epoch 11, Batch 20, Loss: 5.0306\n",
            "Epoch 11, Batch 30, Loss: 4.9928\n",
            "Epoch 11, Batch 40, Loss: 4.9413\n",
            "Epoch 11 completed. Average loss: 5.0117\n",
            "Epoch 12, Batch 0, Loss: 5.0063\n",
            "Epoch 12, Batch 10, Loss: 4.9872\n",
            "Epoch 12, Batch 20, Loss: 4.9912\n",
            "Epoch 12, Batch 30, Loss: 4.9521\n",
            "Epoch 12, Batch 40, Loss: 4.9357\n",
            "Epoch 12 completed. Average loss: 4.9744\n",
            "Epoch 13, Batch 0, Loss: 4.9538\n",
            "Epoch 13, Batch 10, Loss: 4.9656\n",
            "Epoch 13, Batch 20, Loss: 4.9748\n",
            "Epoch 13, Batch 30, Loss: 4.9372\n",
            "Epoch 13, Batch 40, Loss: 4.9066\n",
            "Epoch 13 completed. Average loss: 4.9585\n",
            "Epoch 14, Batch 0, Loss: 4.9296\n",
            "Epoch 14, Batch 10, Loss: 4.9310\n",
            "Epoch 14, Batch 20, Loss: 4.9226\n",
            "Epoch 14, Batch 30, Loss: 4.9188\n",
            "Epoch 14, Batch 40, Loss: 4.9220\n",
            "Epoch 14 completed. Average loss: 4.9239\n",
            "Epoch 15, Batch 0, Loss: 4.9227\n",
            "Epoch 15, Batch 10, Loss: 4.9561\n",
            "Epoch 15, Batch 20, Loss: 4.9110\n",
            "Epoch 15, Batch 30, Loss: 4.9224\n",
            "Epoch 15, Batch 40, Loss: 4.8681\n",
            "Epoch 15 completed. Average loss: 4.9063\n",
            "Epoch 16, Batch 0, Loss: 4.8707\n",
            "Epoch 16, Batch 10, Loss: 4.8826\n",
            "Epoch 16, Batch 20, Loss: 4.8508\n",
            "Epoch 16, Batch 30, Loss: 4.8823\n",
            "Epoch 16, Batch 40, Loss: 4.9280\n",
            "Epoch 16 completed. Average loss: 4.8830\n",
            "Epoch 17, Batch 0, Loss: 4.8330\n",
            "Epoch 17, Batch 10, Loss: 4.8304\n",
            "Epoch 17, Batch 20, Loss: 4.8496\n",
            "Epoch 17, Batch 30, Loss: 4.8790\n",
            "Epoch 17, Batch 40, Loss: 4.9694\n",
            "Epoch 17 completed. Average loss: 4.8589\n",
            "Epoch 18, Batch 0, Loss: 4.8299\n",
            "Epoch 18, Batch 10, Loss: 4.8197\n",
            "Epoch 18, Batch 20, Loss: 4.8983\n",
            "Epoch 18, Batch 30, Loss: 4.8358\n",
            "Epoch 18, Batch 40, Loss: 4.8126\n",
            "Epoch 18 completed. Average loss: 4.8599\n",
            "Epoch 19, Batch 0, Loss: 4.8625\n",
            "Epoch 19, Batch 10, Loss: 4.8574\n",
            "Epoch 19, Batch 20, Loss: 4.8948\n",
            "Epoch 19, Batch 30, Loss: 4.8143\n",
            "Epoch 19, Batch 40, Loss: 4.8707\n",
            "Epoch 19 completed. Average loss: 4.8437\n",
            "Epoch 20, Batch 0, Loss: 4.8096\n",
            "Epoch 20, Batch 10, Loss: 4.8176\n",
            "Epoch 20, Batch 20, Loss: 4.8506\n",
            "Epoch 20, Batch 30, Loss: 4.8941\n",
            "Epoch 20, Batch 40, Loss: 4.8548\n",
            "Epoch 20 completed. Average loss: 4.8294\n",
            "Epoch 21, Batch 0, Loss: 4.7721\n",
            "Epoch 21, Batch 10, Loss: 4.7934\n",
            "Epoch 21, Batch 20, Loss: 4.7846\n",
            "Epoch 21, Batch 30, Loss: 4.7245\n",
            "Epoch 21, Batch 40, Loss: 4.8858\n",
            "Epoch 21 completed. Average loss: 4.8164\n",
            "Epoch 22, Batch 0, Loss: 4.8070\n",
            "Epoch 22, Batch 10, Loss: 4.8290\n",
            "Epoch 22, Batch 20, Loss: 4.7888\n",
            "Epoch 22, Batch 30, Loss: 4.7969\n",
            "Epoch 22, Batch 40, Loss: 4.8270\n",
            "Epoch 22 completed. Average loss: 4.8024\n",
            "Epoch 23, Batch 0, Loss: 4.8433\n",
            "Epoch 23, Batch 10, Loss: 4.8055\n",
            "Epoch 23, Batch 20, Loss: 4.8086\n",
            "Epoch 23, Batch 30, Loss: 4.7978\n",
            "Epoch 23, Batch 40, Loss: 4.7769\n",
            "Epoch 23 completed. Average loss: 4.7886\n",
            "Epoch 24, Batch 0, Loss: 4.7153\n",
            "Epoch 24, Batch 10, Loss: 4.7695\n",
            "Epoch 24, Batch 20, Loss: 4.7883\n",
            "Epoch 24, Batch 30, Loss: 4.7841\n",
            "Epoch 24, Batch 40, Loss: 4.7859\n",
            "Epoch 24 completed. Average loss: 4.7748\n",
            "Epoch 25, Batch 0, Loss: 4.8110\n",
            "Epoch 25, Batch 10, Loss: 4.7865\n",
            "Epoch 25, Batch 20, Loss: 4.7355\n",
            "Epoch 25, Batch 30, Loss: 4.8223\n",
            "Epoch 25, Batch 40, Loss: 4.7511\n",
            "Epoch 25 completed. Average loss: 4.7679\n",
            "Epoch 26, Batch 0, Loss: 4.7767\n",
            "Epoch 26, Batch 10, Loss: 4.7540\n",
            "Epoch 26, Batch 20, Loss: 4.7323\n",
            "Epoch 26, Batch 30, Loss: 4.7625\n",
            "Epoch 26, Batch 40, Loss: 4.7054\n",
            "Epoch 26 completed. Average loss: 4.7630\n",
            "Epoch 27, Batch 0, Loss: 4.7820\n",
            "Epoch 27, Batch 10, Loss: 4.7346\n",
            "Epoch 27, Batch 20, Loss: 4.7332\n",
            "Epoch 27, Batch 30, Loss: 4.7608\n",
            "Epoch 27, Batch 40, Loss: 4.7582\n",
            "Epoch 27 completed. Average loss: 4.7483\n",
            "Epoch 28, Batch 0, Loss: 4.7619\n",
            "Epoch 28, Batch 10, Loss: 4.6709\n",
            "Epoch 28, Batch 20, Loss: 4.7659\n",
            "Epoch 28, Batch 30, Loss: 4.7470\n",
            "Epoch 28, Batch 40, Loss: 4.6631\n",
            "Epoch 28 completed. Average loss: 4.7467\n",
            "Epoch 29, Batch 0, Loss: 4.7379\n",
            "Epoch 29, Batch 10, Loss: 4.7430\n",
            "Epoch 29, Batch 20, Loss: 4.7158\n",
            "Epoch 29, Batch 30, Loss: 4.7854\n",
            "Epoch 29, Batch 40, Loss: 4.7543\n",
            "Epoch 29 completed. Average loss: 4.7324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and configuration\n",
        "os.makedirs(\"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/config\", exist_ok=True)\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/time_series_model.pth\"\n",
        "config_path = \"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/config\"\n",
        "\n",
        "# Get the unwrapped model if using accelerator\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "\n",
        "# Save model state dictionary\n",
        "torch.save(unwrapped_model.state_dict(), model_path)\n",
        "\n",
        "# Save the configuration\n",
        "unwrapped_model.config.to_json_file(os.path.join(\"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/config/config.json\"))\n",
        "\n",
        "# Save frequency and prediction length for later use\n",
        "with open(os.path.join(\"/content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/config/metadata.txt\"), \"w\") as f:\n",
        "    f.write(f\"freq={freq}\\n\")\n",
        "    f.write(f\"prediction_length={prediction_length}\\n\")\n",
        "    f.write(f\"target_column={target_column}\\n\")\n",
        "    f.write(f\"lags_sequence={lags_sequence}\\n\")\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n",
        "print(f\"Configuration saved to {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh1H3glRVRKI",
        "outputId": "06873a47-abe5-4839-ef32-87cbd00140a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/time_series_model.pth\n",
            "Configuration saved to /content/drive/MyDrive/Colab Notebooks/weather_forecasting/weather_model/config\n"
          ]
        }
      ]
    }
  ]
}