{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksandrZadvornyi/weather-forecasting/blob/main/prepare_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdivx5UdGvLD"
      },
      "outputs": [],
      "source": [
        "!pip install -q dask\n",
        "!pip install -q gluonts ujson\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eul86ODxGD5B"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from datasets import Dataset, Features, Value, Sequence, DatasetDict\n",
        "from gluonts.itertools import Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4tE4qVe_jul"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_HZWvfPGHVH"
      },
      "outputs": [],
      "source": [
        "# When reading the CSV files, specify dtypes\n",
        "dtypes = {\n",
        "    'STATION': 'object',\n",
        "    'DATE': 'object',  # Start as string, convert to datetime later\n",
        "    'LATITUDE': 'float64',\n",
        "    'LONGITUDE': 'float64',\n",
        "    'ELEVATION': 'float64',\n",
        "    'NAME': 'object',\n",
        "    'PRCP': 'object',  # Keep as object initially to handle missing values\n",
        "    'PRCP_ATTRIBUTES': 'object',\n",
        "    'TMAX': 'object',\n",
        "    'TMAX_ATTRIBUTES': 'object',\n",
        "    'TMIN': 'object',\n",
        "    'TMIN_ATTRIBUTES': 'object',\n",
        "    'TAVG': 'object',\n",
        "    'TAVG_ATTRIBUTES': 'object',\n",
        "    'SNWD': 'object',\n",
        "    'SNWD_ATTRIBUTES': 'object',\n",
        "}\n",
        "\n",
        "# 1. Load Ukrainian station data\n",
        "ukrainian_files = glob.glob('/content/drive/MyDrive/weather_forecasting_project/daily-summaries-latest-upm/*.csv')\n",
        "print(f\"Found {len(ukrainian_files)} data files\")\n",
        "\n",
        "# Set files count limit\n",
        "max_files = 20\n",
        "ukrainian_files = ukrainian_files[:max_files]\n",
        "print(f\"Reading {len(ukrainian_files)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-29dLOBLkvX"
      },
      "outputs": [],
      "source": [
        "# 2. Read and combine files\n",
        "ddf = dd.read_csv(ukrainian_files, dtype=dtypes, assume_missing=True)\n",
        "print(f\"Data loaded with {len(ddf.columns)} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XzJ4hukGZlP"
      },
      "outputs": [],
      "source": [
        "# 3. Apply preprocessing to sample for metadata\n",
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "\n",
        "    # Ensure all expected columns exist, filling with NaN if necessary\n",
        "    expected_columns = ['PRCP', 'TMAX', 'TMIN', 'TAVG']\n",
        "    for col in expected_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan  # Ensure column exists\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Convert temperature from tenths of degrees Celsius to Celsius\n",
        "    temperature_columns = ['TMAX', 'TMIN', 'TAVG']\n",
        "    for col in temperature_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col] / 10.0\n",
        "\n",
        "    if 'TMAX' in df.columns and 'TMIN' in df.columns:\n",
        "        df['TEMP_RANGE'] = df['TMAX'] - df['TMIN']\n",
        "\n",
        "    df['MONTH'] = df['DATE'].dt.month\n",
        "    df['DAY'] = df['DATE'].dt.day\n",
        "    df['YEAR'] = df['DATE'].dt.year\n",
        "    df['DAY_OF_YEAR'] = df['DATE'].dt.dayofyear\n",
        "    df['SEASON'] = ((df['MONTH'] % 12) // 3 + 1).astype(int)\n",
        "\n",
        "    cols_to_drop = [col for col in df.columns if '_ATTRIBUTES' in col]\n",
        "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "    # Ensure column order matches the metadata\n",
        "    expected_order = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME',\n",
        "                      'PRCP', 'TMAX', 'TMIN', 'TAVG', 'TEMP_RANGE',\n",
        "                      'MONTH', 'DAY', 'YEAR', 'DAY_OF_YEAR', 'SEASON']\n",
        "\n",
        "    df = df.reindex(columns=expected_order)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_station_data(station_df):\n",
        "    station_df = station_df.sort_values('DATE')\n",
        "\n",
        "    if len(station_df) < 365:\n",
        "        return None\n",
        "\n",
        "    station_meta = station_df.iloc[0][['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].to_dict()\n",
        "\n",
        "    station_df = station_df.set_index('DATE').loc[~station_df.index.duplicated(keep='first')]\n",
        "\n",
        "    date_range = pd.date_range(start=station_df.index.min(), end=station_df.index.max(), freq='D')\n",
        "    resampled_df = station_df.reindex(date_range)\n",
        "\n",
        "    numeric_cols = ['PRCP', 'TMAX', 'TMIN', 'TAVG', 'TEMP_RANGE']\n",
        "    numeric_cols = [col for col in numeric_cols if col in resampled_df.columns]\n",
        "\n",
        "    def seasonal_interpolation(series):\n",
        "        interpolated = series.interpolate(method='linear', limit=7)\n",
        "        return interpolated.fillna(series.groupby(series.index.month).transform('median'))\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        resampled_df[col] = resampled_df.groupby(resampled_df.index.month)[col].transform(seasonal_interpolation)\n",
        "\n",
        "        if col in ['TMAX', 'TMIN', 'TAVG']:\n",
        "            resampled_df[col] = resampled_df[col].fillna(resampled_df[col].rolling(window=15, center=True, min_periods=1).median())\n",
        "\n",
        "        elif col == 'PRCP':\n",
        "            log_interpolated = np.log1p(resampled_df[col] + 0.01).interpolate(method='cubic', limit=10)\n",
        "            resampled_df[col] = np.maximum(np.expm1(log_interpolated), 0)\n",
        "\n",
        "        resampled_df[col] = resampled_df[col].ffill(limit=60).bfill(limit=60)\n",
        "\n",
        "        global_median = station_df[col].median()\n",
        "        resampled_df[col] = resampled_df[col].fillna(global_median)\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in ['TMAX', 'TMIN', 'TAVG']:\n",
        "            Q1, Q3 = resampled_df[col].quantile([0.25, 0.75])\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "            resampled_df[col] = np.where((resampled_df[col] < lower_bound) | (resampled_df[col] > upper_bound), global_median, resampled_df[col])\n",
        "\n",
        "        elif col == 'PRCP':\n",
        "            resampled_df[col] = np.maximum(resampled_df[col], 0)\n",
        "\n",
        "    for key, value in station_meta.items():\n",
        "        resampled_df[key] = value\n",
        "\n",
        "    resampled_df = resampled_df.reset_index().rename(columns={'index': 'DATE'})\n",
        "    resampled_df['MONTH'], resampled_df['DAY'], resampled_df['YEAR'] = resampled_df['DATE'].dt.month, resampled_df['DATE'].dt.day, resampled_df['DATE'].dt.year\n",
        "    resampled_df['DAY_OF_YEAR'] = resampled_df['DATE'].dt.dayofyear\n",
        "    resampled_df['SEASON'] = ((resampled_df['MONTH'] % 12) // 3 + 1).astype(int)\n",
        "\n",
        "    if 'TMAX' in resampled_df.columns and 'TMIN' in resampled_df.columns:\n",
        "        resampled_df['TEMP_RANGE'] = resampled_df['TMAX'] - resampled_df['TMIN']\n",
        "\n",
        "    # Final NaN check and global median replacement\n",
        "    for col in numeric_cols:\n",
        "        if resampled_df[col].isna().any():\n",
        "            # print(f\"Warning: Remaining NaNs in {col} column\")\n",
        "            global_median = station_df[col].median()\n",
        "            resampled_df[col] = resampled_df[col].fillna(global_median)\n",
        "\n",
        "\n",
        "    return resampled_df\n",
        "\n",
        "# Create metadata by applying function to the first partition\n",
        "meta_df = preprocess(ddf.partitions[0].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzBMKvMJF8uf"
      },
      "outputs": [],
      "source": [
        "# 4. Apply preprocessing with metadata\n",
        "print(\"Preprocessing data...\")\n",
        "ddf = ddf.map_partitions(preprocess, meta=meta_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiQ_8_gdo2fL"
      },
      "outputs": [],
      "source": [
        "# 5. Convert Dask dataframe to Pandas for further processing\n",
        "# This will load all data into memory (ensure you have enough RAM)\n",
        "print(\"Converting to pandas dataframe...\")\n",
        "df = ddf.compute()\n",
        "\n",
        "target_column = 'TMAX'\n",
        "\n",
        "# Drop rows with NaN values\n",
        "print(\"Dropping rows with NaN values...\")\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "df = df.dropna(subset=[target_column])\n",
        "print(f\"Rows remaining after dropping NaNs: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wHTqUi4o0bl"
      },
      "outputs": [],
      "source": [
        "# 6. Create a time-indexed dataset\n",
        "# Group by station and apply resampling\n",
        "print(\"Resampling data by station...\")\n",
        "processed_stations = []\n",
        "for station_id, station_df in df.groupby('STATION'):\n",
        "    try:\n",
        "        processed_station = process_station_data(station_df)\n",
        "        if processed_station is not None:\n",
        "            processed_stations.append(processed_station)\n",
        "            print(f\"Processed station {station_id} with {len(processed_station)} days of data\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing station {station_id}: {e}\")\n",
        "\n",
        "# Combine all processed stations\n",
        "print(\"Combining processed stations...\")\n",
        "if processed_stations:\n",
        "    processed_df = pd.concat(processed_stations)\n",
        "    print(f\"Combined dataset has {len(processed_df)} rows\")\n",
        "else:\n",
        "    raise ValueError(\"No stations were successfully processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1BVdbLbowVx"
      },
      "outputs": [],
      "source": [
        "# 7. Split data into training/validation/test sets based on time\n",
        "print(\"Splitting data into train/validation/test sets...\")\n",
        "\n",
        "# For each station, perform temporal split\n",
        "train_data = {}\n",
        "val_data = {}\n",
        "test_data = {}\n",
        "\n",
        "def split_time_series(processed_df, train_ratio=0.7, val_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Split dataframe by station into train/val/test sets based on time,\n",
        "    preserving proper date information.\n",
        "    \"\"\"\n",
        "    train_data = {}\n",
        "    val_data = {}\n",
        "    test_data = {}\n",
        "\n",
        "    for station_id, station_df in processed_df.groupby('STATION'):\n",
        "        # Ensure proper datetime column\n",
        "        if not pd.api.types.is_datetime64_any_dtype(station_df['DATE']):\n",
        "            station_df['DATE'] = pd.to_datetime(station_df['DATE'])\n",
        "\n",
        "        # Sort by date\n",
        "        station_df = station_df.sort_values('DATE')\n",
        "\n",
        "        # Calculate split points\n",
        "        n = len(station_df)\n",
        "        train_end = int(n * train_ratio)\n",
        "        val_end = int(n * (train_ratio + val_ratio))\n",
        "\n",
        "        # Split the data\n",
        "        train = station_df.iloc[:train_end].copy()\n",
        "        val = station_df.iloc[train_end:val_end].copy()\n",
        "        test = station_df.iloc[val_end:].copy()\n",
        "\n",
        "        # Store in dictionaries\n",
        "        train_data[station_id] = train\n",
        "        val_data[station_id] = val\n",
        "        test_data[station_id] = test\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "train_data, val_data, test_data = split_time_series(processed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jh1H3glRVRKI"
      },
      "outputs": [],
      "source": [
        "# 8. Prepare data for GluonTS format\n",
        "# First, let's convert the data into the appropriate format for each split\n",
        "print(\"Converting data to GluonTS format...\")\n",
        "\n",
        "def prepare_gluonts_data(data_dict, target_column='TMAX'):\n",
        "    \"\"\"\n",
        "    Convert dictionary of dataframes to GluonTS dataset format.\n",
        "\n",
        "    Args:\n",
        "        data_dict: Dictionary of dataframes, keyed by station_id\n",
        "        target_column: Column to use as target variable\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries in GluonTS format\n",
        "    \"\"\"\n",
        "    gluonts_data = []\n",
        "\n",
        "    # Create a mapping of station IDs to unique categorical indices\n",
        "    station_ids = list(data_dict.keys())\n",
        "    station_id_to_cat = {station_id: idx for idx, station_id in enumerate(station_ids)}\n",
        "\n",
        "    for station_id, df in data_dict.items():\n",
        "        # Ensure the dataframe has a proper date index or DATE column\n",
        "        if 'DATE' in df.columns:\n",
        "            # If DATE is a column, ensure it's a datetime\n",
        "            if not pd.api.types.is_datetime64_any_dtype(df['DATE']):\n",
        "                df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "\n",
        "            # Sort by DATE\n",
        "            df = df.sort_values('DATE')\n",
        "\n",
        "            # Get start timestamp as a proper pandas Timestamp\n",
        "            start = df['DATE'].iloc[0]\n",
        "        else:\n",
        "            # If using an index, ensure it's a datetime index\n",
        "            if not isinstance(df.index, pd.DatetimeIndex):\n",
        "                df.index = pd.to_datetime(df.index)\n",
        "\n",
        "            # Sort by date index\n",
        "            df = df.sort_index()\n",
        "\n",
        "            # Get start timestamp\n",
        "            start = df.index[0]\n",
        "\n",
        "        # Convert start to string format that GluonTS can handle\n",
        "        # Format: YYYY-MM-DD HH:MM:SS\n",
        "        start_str = start.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Extract target variable\n",
        "        target = df[target_column].values.astype(np.float32)\n",
        "\n",
        "        # Create static categorical feature\n",
        "        static_cat = [station_id_to_cat[station_id]]\n",
        "\n",
        "        # Create optional static real features\n",
        "        static_real = [\n",
        "            df['LATITUDE'].iloc[0],\n",
        "            df['LONGITUDE'].iloc[0],\n",
        "            df['ELEVATION'].iloc[0]\n",
        "        ]\n",
        "\n",
        "        # Create dictionary in GluonTS format\n",
        "        ts_data = {\n",
        "            \"start\": start_str,  # Store as string for consistent handling\n",
        "            \"target\": target,\n",
        "            \"feat_static_cat\": static_cat,\n",
        "            \"feat_static_real\": static_real,\n",
        "            \"item_id\": station_id\n",
        "        }\n",
        "\n",
        "        gluonts_data.append(ts_data)\n",
        "\n",
        "    return gluonts_data\n",
        "\n",
        "# Choose target variable\n",
        "#target_column = 'TMIN'  # Can be changed to TMIN, PRCP, etc.\n",
        "\n",
        "# Convert data to GluonTS format\n",
        "train_gluonts = prepare_gluonts_data(train_data, target_column)\n",
        "val_gluonts = prepare_gluonts_data(val_data, target_column)\n",
        "test_gluonts = prepare_gluonts_data(test_data, target_column)\n",
        "\n",
        "print(f\"Created {len(train_gluonts)} training series, {len(val_gluonts)} validation series, {len(test_gluonts)} test series\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mib6WQpruR7x"
      },
      "outputs": [],
      "source": [
        "# 9. Create HuggingFace Datasets\n",
        "print(\"Creating HuggingFace Datasets...\")\n",
        "\n",
        "# Process start field for HuggingFace Dataset\n",
        "class ProcessStartField:\n",
        "    ts_id = 0\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # Convert start from string to datetime\n",
        "        if isinstance(data[\"start\"], str):\n",
        "            data[\"start\"] = pd.Timestamp(data[\"start\"])\n",
        "\n",
        "        # Ensure start is a timestamp\n",
        "        if isinstance(data[\"start\"], pd.Timestamp):\n",
        "            data[\"start\"] = data[\"start\"].to_pydatetime()\n",
        "\n",
        "        # Assign a unique ID to each time series\n",
        "        if \"feat_static_cat\" not in data:\n",
        "            data[\"feat_static_cat\"] = [self.ts_id]\n",
        "\n",
        "        self.ts_id += 1\n",
        "        return data\n",
        "\n",
        "# Apply processing to each split\n",
        "process_start = ProcessStartField()\n",
        "train_processed = list(Map(process_start, train_gluonts))\n",
        "process_start = ProcessStartField()  # Reset ID counter\n",
        "val_processed = list(Map(process_start, val_gluonts))\n",
        "process_start = ProcessStartField()  # Reset ID counter\n",
        "test_processed = list(Map(process_start, test_gluonts))\n",
        "\n",
        "# Define dataset features\n",
        "features = Features(\n",
        "    {\n",
        "        \"start\": Value(\"timestamp[s]\"),\n",
        "        \"target\": Sequence(Value(\"float32\")),\n",
        "        \"feat_static_cat\": Sequence(Value(\"int64\")),\n",
        "        \"feat_static_real\": Sequence(Value(\"float32\")),\n",
        "        \"item_id\": Value(\"string\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Create HuggingFace Datasets\n",
        "train_dataset = Dataset.from_list(train_processed, features=features)\n",
        "val_dataset = Dataset.from_list(val_processed, features=features)\n",
        "test_dataset = Dataset.from_list(test_processed, features=features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOYoxkkIuUQU"
      },
      "outputs": [],
      "source": [
        "# 10. Save the prepared datasets\n",
        "print(\"Saving prepared datasets...\")\n",
        "data_dir = \"/content/drive/MyDrive/weather_forecasting_project/prepared_datasets\"\n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Create a DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "# Save the DatasetDict\n",
        "dataset.save_to_disk(f\"{data_dir}/dataset\")\n",
        "\n",
        "# Save target column information\n",
        "with open(f\"{data_dir}/metadata.txt\", \"w\") as f:\n",
        "    f.write(f\"target_column={target_column}\\n\")\n",
        "    f.write(\"freq=D\\n\")  # Daily frequency\n",
        "\n",
        "print(\"Data preparation complete!\")\n",
        "print(\"Datasets saved to:\", data_dir)\n",
        "print(\"Final dataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)} series\")\n",
        "print(f\"Validation: {len(val_dataset)} series\")\n",
        "print(f\"Test: {len(test_dataset)} series\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHGVVFd8aLtm"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][\"item_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf3X788BZzle"
      },
      "outputs": [],
      "source": [
        "print(pd.Timestamp(train_dataset[0][\"start\"]))\n",
        "print(pd.Timestamp(val_dataset[0][\"start\"]))\n",
        "print(pd.Timestamp(test_dataset[0][\"start\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO9_E5hduWsd"
      },
      "outputs": [],
      "source": [
        "def plot_sequential_time_series(train_example, validation_example, test_example,\n",
        "                             title=None, target_column=\"TMAX\"):\n",
        "    \"\"\"\n",
        "    Plot time series data sequentially from train, validation, and test sets with datetime on x-axis.\n",
        "    \"\"\"\n",
        "    # Create a figure and axis\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Extract target values\n",
        "    train_target = train_example[\"target\"]\n",
        "    validation_target = validation_example[\"target\"]\n",
        "    test_target = test_example[\"target\"]\n",
        "\n",
        "    # Get start dates - parse with pandas to ensure proper handling\n",
        "    train_start = pd.Timestamp(train_example[\"start\"])\n",
        "    val_start = pd.Timestamp(validation_example[\"start\"])\n",
        "    test_start = pd.Timestamp(test_example[\"start\"])\n",
        "\n",
        "    # Debugging info\n",
        "    print(f\"Train start: {train_start}, length: {len(train_target)}\")\n",
        "    print(f\"Val start: {val_start}, length: {len(validation_target)}\")\n",
        "    print(f\"Test start: {test_start}, length: {len(test_target)}\")\n",
        "\n",
        "    # Create date ranges for each dataset\n",
        "    train_dates = pd.date_range(start=train_start, periods=len(train_target), freq='D')\n",
        "    val_dates = pd.date_range(start=val_start, periods=len(validation_target), freq='D')\n",
        "    test_dates = pd.date_range(start=test_start, periods=len(test_target), freq='D')\n",
        "\n",
        "    # Plot each array with actual dates\n",
        "    plt.plot(train_dates, train_target, color=\"blue\", label=\"Train\")\n",
        "    plt.plot(val_dates, validation_target, color=\"red\", label=\"Validation\")\n",
        "    plt.plot(test_dates, test_target, color=\"green\", label=\"Test\")\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(f\"{target_column} Temperature\")\n",
        "    plt.title(title or f\"Time Series Plot of {target_column} ({train_example['item_id']})\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Add vertical lines to separate datasets\n",
        "    if len(train_dates) > 0:\n",
        "        plt.axvline(x=train_dates[-1], color='gray', linestyle='--', alpha=0.7)\n",
        "    if len(val_dates) > 0:\n",
        "        plt.axvline(x=val_dates[-1], color='gray', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Format x-axis dates\n",
        "    plt.gcf().autofmt_xdate()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage in your existing code:\n",
        "for x in range(max_files // 10):\n",
        "    plot_sequential_time_series(\n",
        "        train_dataset[x],\n",
        "        val_dataset[x],\n",
        "        test_dataset[x],\n",
        "        target_column=target_column\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
